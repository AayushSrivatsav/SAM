{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm64FORyoaOw"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q datasets\n",
        "!pip install -q monai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2g-hvcpD5iUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset\n",
        "\n",
        "Here we load a small dataset of 130 (image, ground truth mask) pairs.\n"
      ],
      "metadata": {
        "id": "ROd15m4Ucdut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"nielsr/breast-cancer\", split=\"train\")"
      ],
      "metadata": {
        "id": "bCWVOTqn5IA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "Yg9K8bcQ5bwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset[0]\n",
        "image = example[\"image\"]\n",
        "image"
      ],
      "metadata": {
        "id": "Clbqmjop5ZI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "fig, axes = plt.subplots()\n",
        "\n",
        "axes.imshow(np.array(image))\n",
        "ground_truth_seg = np.array(example[\"label\"])\n",
        "show_mask(ground_truth_seg, axes)\n",
        "axes.title.set_text(f\"Ground truth mask\")\n",
        "axes.axis(\"off\")"
      ],
      "metadata": {
        "id": "bu1YYC2Q6MmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9TgyvFAV70-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create PyTorch dataset\n",
        "\n",
        "Below defining a regular PyTorch dataset, which gives us examples of the data prepared in the format for the model. Each example consists of:\n",
        "\n",
        "* pixel values (which is the image prepared for the model)\n",
        "* a prompt in the form of a bounding box\n",
        "* a ground truth segmentation mask.\n",
        "\n",
        "SAM is always trained using certain \"prompts\", which you could be bounding boxes, points, text, or rudimentary masks. The model is then trained to output the appropriate mask given the image + prompt."
      ],
      "metadata": {
        "id": "J2cOa80Qc3FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bounding_box(ground_truth_map):\n",
        "  # get bounding box from mask\n",
        "  y_indices, x_indices = np.where(ground_truth_map > 0)\n",
        "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
        "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
        "  # add perturbation to bounding box coordinates\n",
        "  H, W = ground_truth_map.shape\n",
        "  x_min = max(0, x_min - np.random.randint(0, 20))\n",
        "  x_max = min(W, x_max + np.random.randint(0, 20))\n",
        "  y_min = max(0, y_min - np.random.randint(0, 20))\n",
        "  y_max = min(H, y_max + np.random.randint(0, 20))\n",
        "  bbox = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "  return bbox"
      ],
      "metadata": {
        "id": "fGGCEE8D7vnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SAMDataset(Dataset):\n",
        "  def __init__(self, dataset, processor):\n",
        "    self.dataset = dataset\n",
        "    self.processor = processor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.dataset[idx]\n",
        "    image = item[\"image\"]\n",
        "    ground_truth_mask = np.array(item[\"label\"])\n",
        "\n",
        "    # get bounding box prompt\n",
        "    prompt = get_bounding_box(ground_truth_mask)\n",
        "\n",
        "    # prepare image and prompt for the model\n",
        "    inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
        "\n",
        "    # remove batch dimension which the processor adds by default\n",
        "    inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
        "\n",
        "    # add ground truth segmentation\n",
        "    inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
        "\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "oadZuVUp8J7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SamProcessor\n",
        "\n",
        "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
      ],
      "metadata": {
        "id": "SpIOOyef8NXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SAMDataset(dataset=dataset, processor=processor)"
      ],
      "metadata": {
        "id": "VT9x-EvC8hXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = train_dataset[0]\n",
        "for k,v in example.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "aIVY31V08kYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define a PyTorch Dataloader, which allows us to get batches from the dataset."
      ],
      "metadata": {
        "id": "dxqnwCOe8pdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "2b5LCifU8oc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "for k,v in batch.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "-leJ4wnb9MzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"ground_truth_mask\"].shape"
      ],
      "metadata": {
        "id": "1oQjRoTL9NvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import SamModel\n",
        "\n",
        "model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n",
        "\n",
        "# make sure we only compute gradients for mask decoder\n",
        "for name, param in model.named_parameters():\n",
        "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
        "    param.requires_grad_(False)"
      ],
      "metadata": {
        "id": "F229pdUu9Qhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "BiNHxPNL9Y3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import monai\n",
        "\n",
        "# Note: Hyperparameter tuning could improve performance here\n",
        "optimizer = Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
        "\n",
        "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
      ],
      "metadata": {
        "id": "iREMZl2X9Uyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from statistics import mean\n",
        "import torch\n",
        "from torch.nn.functional import threshold, normalize\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_losses = []\n",
        "    for batch in tqdm(train_dataloader):\n",
        "      # forward pass\n",
        "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
        "                      input_boxes=batch[\"input_boxes\"].to(device),\n",
        "                      multimask_output=False)\n",
        "\n",
        "      # compute loss\n",
        "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
        "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
        "      loss = seg_loss(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
        "\n",
        "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # optimize\n",
        "      optimizer.step()\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "    print(f'EPOCH: {epoch}')\n",
        "    print(f'Mean loss: {mean(epoch_losses)}')"
      ],
      "metadata": {
        "id": "QwE98oav9b91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9eHwEpo69_Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Important note here: as we used the Dice loss with `sigmoid=True`, we need to make sure to appropriately apply a sigmoid activation function to the predicted masks. Hence we won't use the processor's `post_process_masks` method here."
      ],
      "metadata": {
        "id": "IYKvlFthRsyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# let's take a random training example\n",
        "idx = 10\n",
        "\n",
        "# load image\n",
        "image = dataset[idx][\"image\"]\n",
        "image"
      ],
      "metadata": {
        "id": "dnq-EYcG95yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get box prompt based on ground truth segmentation map\n",
        "ground_truth_mask = np.array(dataset[idx][\"label\"])\n",
        "prompt = get_bounding_box(ground_truth_mask)\n",
        "\n",
        "# prepare image + box prompt for the model\n",
        "inputs = processor(image, input_boxes=[[prompt]], return_tensors=\"pt\").to(device)\n",
        "for k,v in inputs.items():\n",
        "  print(k,v.shape)"
      ],
      "metadata": {
        "id": "MsN68aFn-CL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs, multimask_output=False)"
      ],
      "metadata": {
        "id": "ZRP8M37c-FL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply sigmoid\n",
        "medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
        "# convert soft mask to hard mask\n",
        "medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
        "medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)"
      ],
      "metadata": {
        "id": "y0tHnwBC-IPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "fig, axes = plt.subplots()\n",
        "\n",
        "axes.imshow(np.array(image))\n",
        "show_mask(medsam_seg, axes)\n",
        "axes.title.set_text(f\"Predicted mask\")\n",
        "axes.axis(\"off\")"
      ],
      "metadata": {
        "id": "DQpnuH2J-LYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots()\n",
        "\n",
        "axes.imshow(np.array(image))\n",
        "show_mask(ground_truth_mask, axes)\n",
        "axes.title.set_text(f\"Ground truth mask\")\n",
        "axes.axis(\"off\")"
      ],
      "metadata": {
        "id": "swGs0mSn-RAE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}